Synthetic Data Generation / Generating Question Answer pairs for Supervised Learning are created by manually passing a modified generated prompt into ChatGpt on gpt-4 as low level models were unable to prvide valid JSON and facing token limit problem.

MODIFIED GENERATED_PROMPT :------------------------------------
(variables = entries_num, n)
You are an expert data curator assisting a machine learning engineer in creating a high-quality instruction tuning dataset. Your task is to transform 
    the provided data chunk into diverse question and answer (Q&A) pairs that will be used to fine-tune a language model. 

    For each of the {entries_num} entries, generate one or two well-structured questions that reflect different aspects of the information in the chunk.  
    Ensure a mix of longer and shorter questions, with shorter ones typically containing 1-2 sentences and longer ones spanning up to 3-4 sentences. Each 
    Q&A pair should be concise yet informative, capturing key insights from the data.

    Structure your output in JSON format, where each object contains 'question' and 'answer' fields. The JSON structure should look like this:

        "question": "Your question here...",
        "answer": "Your answer here..."

    Focus on creating clear, relevant, and varied questions that encourage the model to learn from diverse perspectives. Avoid any sensitive or biased 
    content, ensuring answers are accurate and neutral.

    Example:
    
        "question": "What is the primary purpose of this dataset?",
        "answer": "This dataset serves as training data for fine-tuning a language model."
    

    By following these guidelines, you'll contribute to a robust and effective dataset that enhances the model's performance."

    ---

    **Explanation:**

    - **Clarity and Specificity:** The revised prompt clearly defines the role of the assistant and the importance of the task, ensuring alignment with the 
    project goals.
    - **Quality Standards:** It emphasizes the need for well-formulated Q&A pairs, specifying the structure and content of each question and answer.
    - **Output Format:** An example JSON structure is provided to guide the format accurately.
    - **Constraints and Biases:** A note on avoiding sensitive or biased content ensures ethical considerations are met.
    - **Step-by-Step Guidance:** The prompt breaks down the task into manageable steps, making it easier for the assistant to follow.

    This approach ensures that the generated data is both high-quality and meets the specific requirements of the machine learning project.
    
    Data is provided as the pdf attached

Divide the data into {n} parts while each part having 5  questions each making a total of 40 question answer pairs

also dont include name of each part in the json just provide 40 question answer pairs.

export part 1-{n} all in one file 

Export a valid json file with no preamble 

-----------------------------------------------------------------

preprocessing.py was not needed here as we stored data directly in question answer pairs only

-----------------------------------------------------------------

instruction is more detailed version of tm1_database.json as it contains context in every question.
I'm not doing that here to save time.

----------------------------------------------------------------

I used train.ipynb to leverage the T4 GPU of google colab but train.py also works fine (if you have CUDA setup then change the commented code)

----------------------------------------------------------------

We obtain adapter_config.json, adapter_model.safetytensors and other metadeta,
we use it to create a model using ollama with the file Modelfile and command

ollama create <name> -f Modelfile

----------------------------------------------------------------

You can see the model using 
    >> ollama list

and use the model with the command
    >> ollama run <name>

----------------------------------------------------------------

